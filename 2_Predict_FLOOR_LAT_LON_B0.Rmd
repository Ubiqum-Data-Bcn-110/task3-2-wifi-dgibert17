---
title: "Wi-Fi positioning system. FLOOR/LAT/LONG predictions on Building 0"
author: "David Gibert Bosque"
date: "January 4th, 2019"
output:
  rmdformats::readthedown:
    self_contained: false
    thumbnails: true
    lightbox: true
    gallery: true
    highlight: tango
---
```{r include=FALSE}
rm(list = ls())
```

```{r message=FALSE, warning=FALSE, include=FALSE}
library(dplyr)
library(Metrics)
library(rpart)
library(rpart.plot)
library(fastDummies)
library(ggplot2)
library(elasticnet)
library(randomForest)
library(plot3D)
library(lars)
library(scatterplot3d)
library(caret)
library(AppliedPredictiveModeling)
library(RColorBrewer)
library(e1071)
library(pls)
library(plotly)
library(class)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
### LOADING DATA ###
setwd("C:/Users/David/Google Drive/Github/task3-2-wifi-dgibert17")

load(file = "training.Rdata")
load(file = "validation.Rdata")
```

**Bold text in English.**

*Texto en cursiva en EspaÃ±ol.*

## VALIDATION & TRAINING datasets for building 0
```{r}
df.train.B0 = df.train %>%
  filter(BUILDINGID == 0)

df.val.B0 = df.val %>%
  filter(BUILDINGID == 0)
```

**Because we have filtered the data by building, we have to do once again the pre-processing carried out previously. Removing undetected WAP's and locate the intersection of attributes of both data sets is a must before starting the machine learning process.**

*Debido a que hemos filtrado por edificio, tenemos que volver a hacer el pre-procesado de datos que hemos llevado a cabo anteriormente. Hay que quitar WAPs no detectados y localizar la interseccion de atributos de ambos datasets antes de empezar con el procedimiento de machine learning.*

---

## Non detected WAP's

**Locating WAP's that have not been detected by any device.**

*Localizamos los WAPs que no han sido detectados por ningun dispositivo.*
```{r}
WAPnotDetected.val = as.character()
WAPnotDetected.tr = as.character()

# VALIDATION DF

for (i in 1:which(names(df.val.B0)=="WAP508")){
  if (sum(df.val.B0[i]) == 100*nrow(df.val.B0)){
    #cat("El", names(df.val[i]), "no ha sido detectado por ningun dispositivo en el validation\n")
    WAPnotDetected.val = c(WAPnotDetected.val, names(df.val.B0[i]))
  }
}

# TRAINING DF

for (i in 1:which(names(df.train.B0)=="WAP508")){
  if (sum(df.train.B0[i]) == 100*nrow(df.train.B0)){
    #cat("El", names(df.train[i]), "no ha sido detectado por ningun dispositivo en el training\n")
    WAPnotDetected.tr = c(WAPnotDetected.tr, names(df.train.B0[i]))
  }
}
```

## Removing non detected WAP's
```{r}
WAPnotDetected.val
# which(names(df.val.B0) %in% WAPnotDetected.val) #Estos son los indices de los atributos que no son detectados en el validation set, por lo que quitaremos estos atributos del validation set.

df.val.B0 = df.val.B0[, -which(names(df.val.B0) %in% WAPnotDetected.val)]
df.train.B0 = df.train.B0[, -which(names(df.train.B0) %in% WAPnotDetected.tr)]
```

---

**Looking for the intersect of attributes appearing in both datasets. Attributes in VALIDATION that appear in TRAINING and vice versa. VALIDATION attributes that appear in TRAINING first.**

*Buscamos la interseccion de los atributos que aparecen en ambos conjuntos de datos. Atributos en VALILDATION que aparecen en TRAINING y viceversa. Los atributos de VALIDATION que aparecen en TRAINING primero.*
```{r}
tr.in.val.idxs = which(names(df.train.B0) %in% names(df.val.B0))
df.train.B0 = df.train.B0[, tr.in.val.idxs]

val.in.tr.idxs = which(names(df.val.B0) %in% names(df.train.B0))
df.val.B0 = df.val.B0[, val.in.tr.idxs]

sum(names(df.train.B0) == names(df.val.B0))

# intersect(x = names(df.train), y = names(df.val))
```

#### Formatting categorical data
```{r}
df.train.B0$FLOOR = factor(df.train.B0$FLOOR)
df.train.B0$BUILDINGID = factor(df.train.B0$BUILDINGID)

df.val.B0$FLOOR = factor(df.val.B0$FLOOR)
df.val.B0$BUILDINGID = factor(df.val.B0$BUILDINGID)

real.values = data.frame(df.val.B0[,c(140:142)])
real.values$TYPE = "Real"

str(df.train.B0[,c(140:149)])
str(df.val.B0[,c(140:149)])
```

---

## Data partition by Floor for Building 0
```{r}
floor.vec = createDataPartition(y = df.train.B0$FLOOR, times = 5, p = 0.1)

# Training data
floor.tr = df.train.B0[, c(1:last(grep(pattern = "WAP", names(df.train.B0))), which(names(df.train.B0) == "FLOOR"))]
floor.tr$FLOOR = factor(floor.tr$FLOOR)

#Data proportion from TRAINING (total) and VALIDATION (sample)
prop.table(table(df.train.B0$FLOOR))
prop.table(table(floor.tr$FLOOR))
```

## Floor prediction - KNN
```{r warning=FALSE}
knn.fit.floor = knn(train = floor.tr[,1:139], test = df.val.B0[,c(1:139)], cl = floor.tr[,140], k = 5)
confusionMatrix(data = factor(knn.fit.floor), reference = df.val.B0$FLOOR)

#Cuantos dispositivos se han clasificado mal por cada edificio.
abs(table(knn.fit.floor) - table(df.val.B0$FLOOR))
```

## Floor prediction - SVM
```{r warning=FALSE}
svm.fit.floor <- svm(formula = FLOOR ~ ., data = floor.tr)
svm.pred.floor <- predict(svm.fit.floor, newdata = df.val.B0[,c(1:139)])

confusionMatrix(data = svm.pred.floor, reference = df.val.B0$FLOOR)

# #Cuantos dispositivos se han clasificado mal por cada edificio.
abs(table(svm.pred.floor) - table(df.val.B0$FLOOR))
```

## Floor prediction - Random Forest
```{r}
# bestmtry = tuneRF(x = floor.tr[,1:139], y = floor.tr$FLOOR, ntreeTry = 1000, plot = T,stepFactor = 1.5, improve = 1e-5)

rf.fit.floor = randomForest(x = floor.tr[,1:139], y = floor.tr$FLOOR, ntree = 200, mtry = 16)
rf.pred.floor = predict(rf.fit.floor, df.val.B0)
confusionMatrix(data = factor(rf.pred.floor), reference = df.val.B0$FLOOR)

#Cuantos dispositivos se han clasificado mal por cada Planta
abs(table(rf.pred.floor) - table(df.val.B0$FLOOR))
```

**The method that produces a prediction with less error is Random Forest.**

*El metodo que produce una prediccion con menor error es Random Forest.*

---

#### Replacing actual Floor values for predicted ones
```{r}
df.val.B0$FLOOR = rf.pred.floor
str(df.val.B0$FLOOR)
```

---

**After making different combinations, the lowest prediction error is achieved by first predicting Latitude with dummified Floor and Longitude later, using the information provided by Latitude.**

*Despues de realizar diferentes combinaciones, el error de prediccion mas bajo se consigue prediciendo primero Latitude con la variable 'Dummy' de Floor y Longitude despues, usando la informacion que proporciona la variable Latitude.*

## Dummifying Floor
```{r}
pacman::p_load(fastDummies)

df.val.B0.dumm = dummy_cols(.data = df.val.B0, select_columns = "FLOOR", remove_first_dummy = FALSE)
df.val.B0.dumm = select(.data = df.val.B0.dumm, -FLOOR)

df.train.B0.dumm = dummy_cols(.data = df.train.B0, select_columns = "FLOOR", remove_first_dummy = FALSE)
df.train.B0.dumm = select(.data = df.train.B0.dumm, -FLOOR)
```

## Latitude prediction - SVM
```{r warning=FALSE}
svm.fit.lat <- svm(formula = LATITUDE ~ ., data = df.train.B0.dumm[,c(1:139, 141, 149:152)])
svm.pred.lat <- predict(svm.fit.lat, newdata = df.val.B0.dumm[,c(1:139, 149:152)])

rmse(actual = df.val.B0.dumm$LATITUDE, predicted = svm.pred.lat)
mae(actual = df.val.B0.dumm$LATITUDE, predicted = svm.pred.lat)
```

## Latitude prediction - Linear Model
```{r warning=FALSE}
lm.fit.lat <- lm(formula = LATITUDE ~ ., data = df.train.B0.dumm[,c(1:139, 141, 149:152)])
lm.pred.lat <- predict(lm.fit.lat, newdata = df.val.B0.dumm[,c(1:139, 149:152)])

rmse(actual = df.val.B0.dumm$LATITUDE, predicted = lm.pred.lat)
mae(actual = df.val.B0.dumm$LATITUDE, predicted = lm.pred.lat)
```

## Latitude prediction - Random Forest
```{r}
# bestmtry = tuneRF(x = df.train.B0.dumm[,c(1:139, 149:152)], y = df.train.B0.dumm$LATITUDE, ntreeTry = 300, plot = T,stepFactor = 1.5, improve = 1e-5)

# rf.fit.long = randomForest(x = df.train.B0.dumm[,c(1:139, 140, 149:152)], y = df.train.B0.dumm$LONGITUDE, ntree = 200, mtry = 22)
rf.fit.lat = randomForest(LATITUDE ~., df.train.B0.dumm[,c(1:139, 141, 149:152)], ntree = 200, mtry = 22)

rf.pred.lat = predict(rf.fit.lat, df.val.B0.dumm[,c(1:139, 149:152)])

rmse(actual = df.val.B0.dumm$LATITUDE, predicted = rf.pred.lat)
mae(actual = df.val.B0.dumm$LATITUDE, predicted = rf.pred.lat)
```

## Latitude prediction - KNN
```{r warning=FALSE}
pacman::p_load(FNN)

knn.fit.lat = knn.reg(train = df.train.B0.dumm[,c(1:139, 149:152)], test = df.val.B0.dumm[,c(1:139, 149:152)],
                       y = df.train.B0.dumm[,141], k = 5,
                       algorithm = c("kd_tree", "cover_tree", "brute"))

rmse(actual = df.val.B0.dumm$LATITUDE, predicted = knn.fit.lat$pred)
mae(actual = df.val.B0.dumm$LATITUDE, predicted = knn.fit.lat$pred)
```

**The method that produces a prediction with less error is Random Forest.**

*El metodo que produce una prediccion con menor error es Random Forest.*

---

#### Replacing actual Latitude values for predicted ones
```{r}
df.val.B0.dumm$LATITUDE = rf.pred.lat
str(df.val.B0.dumm$LATITUDE)
```

---

## Longitude prediction - SVM
```{r warning=FALSE}
svm.fit.long <- svm(formula = LONGITUDE ~ ., data = df.train.B0.dumm[,c(1:139, 140, 141, 149:152)])
svm.pred.long <- predict(svm.fit.long, newdata = df.val.B0.dumm[,c(1:139, 141, 149:152)])

rmse(actual = df.val.B0.dumm$LONGITUDE, predicted = svm.pred.long)
mae(actual = df.val.B0.dumm$LONGITUDE, predicted = svm.pred.long)
```

## Longitude prediction - Linear Model
```{r warning=FALSE}
lm.fit.long <- lm(formula = LONGITUDE ~ ., data = df.train.B0.dumm[,c(1:139, 140, 141, 149:152)])
lm.pred.long <- predict(lm.fit.long, newdata = df.val.B0.dumm[,c(1:139, 141, 149:152)])

rmse(actual = df.val.B0.dumm$LONGITUDE, predicted = lm.pred.long)
mae(actual = df.val.B0.dumm$LONGITUDE, predicted = lm.pred.long)
```

## Longitude prediction - Random Forest
```{r}
# bestmtry = tuneRF(x = df.train.B0.dumm[,c(1:139, 141, 141:144)], y = df.train.B0.dumm$LONGITUDE, ntreeTry = 200, plot = T,stepFactor = 1.5, improve = 1e-5)

rf.fit.long = randomForest(LONGITUDE ~., df.train.B0.dumm[,c(1:139, 140, 141, 149:152)], ntree = 200, mtry = 108)

rf.pred.long = predict(rf.fit.long, df.val.B0.dumm[,c(1:139, 141, 149:152)])

rmse(actual = df.val.B0.dumm$LONGITUDE, predicted = rf.pred.long)
mae(actual = df.val.B0.dumm$LONGITUDE, predicted = rf.pred.long)
```

## Longitude prediction - KNN
```{r warning=FALSE}
pacman::p_load(FNN)

knn.fit.long = knn.reg(train = df.train.B0.dumm[,c(1:139, 141, 149:152)], test = df.val.B0.dumm[,c(1:139, 141, 149:152)],
                       y = df.train.B0.dumm[,140], k = 5,
                       algorithm = c("kd_tree", "cover_tree", "brute"))

rmse(actual = df.val.B0.dumm$LONGITUDE, predicted = knn.fit.long$pred)
mae(actual = df.val.B0.dumm$LONGITUDE, predicted = knn.fit.long$pred)
```

**Random Forest and KNN produce almost the same predictive error rate. Random Forest's predictions are preferred since the MAE level is smaller than KNN's MAE, although RMSE is a bit higher. This is preferable since the RMSE indicates that the predictive error increases in those observations that already have a higher error rate.**

**We would prefer KNN in terms of computational level, since it is much faster than Random Forest in this case.**

*Random Forest y KNN producen practicamente el mismo error de prediccion. Usaremos las predicciones de Random Forest ya que a nivel de MAE es menor que KNN, aunque RMSE es un poco mas alto. Esto es preferible ya que el RMSE nos indica que aumenta el error de prediccion en aquellas observaciones que ya tienen un error de prediccion alto de por si.*

*PreferirÃ­amos KNN en tÃ©rminos de nivel computacional, ya que en este caso es mucho mÃ¡s rÃ¡pido que Random Forest.*

---

#### Replacing actual Longitude values for predicted ones
```{r}
df.val.B0.dumm$LONGITUDE = rf.pred.long
```

---

############## COLUM BIND DATA ##################
######### 3D PLOT VALORES RELES Y PREDICHOS ###########
```{r}
pred.values = data.frame(cbind(df.val.B0.dumm$LONGITUDE, df.val.B0.dumm$LATITUDE, as.factor(df.val.B0$FLOOR)))
colnames(pred.values) = c("pred_LONGITUDE", "pred_LATITUDE", "pred_FLOOR")
colnames(real.values) = c("real_LONGITUDE", "real_LATITUDE", "real_FLOOR", "TYPE")

pred.values$pred_FLOOR = factor(pred.values$pred_FLOOR)
# pred.values$TYPE = "Predicted"

levels(pred.values$pred_FLOOR) = 0:3

str(df.val.B0$FLOOR)
str(pred.values$pred_FLOOR)

real.pred.values = cbind(real.values[,1:3], pred.values)
# real.pred.values$TYPE = factor(real.pred.values$TYPE)

str(real.pred.values)

real.pred.values$long_diff = abs(abs(real.pred.values$real_LONGITUDE) - abs(real.pred.values$pred_LONGITUDE))
real.pred.values$lat_diff = abs(abs(real.pred.values$real_LATITUDE) - abs(real.pred.values$pred_LATITUDE))

plot_ly(real.pred.values) %>%
  add_markers(x = ~real_LONGITUDE, y = ~real_LATITUDE, z = ~real_FLOOR,
              mode = "line", marker = list(size = 3, color = "black"), name = "Real") %>%
  add_markers(x = ~pred_LONGITUDE, y = ~pred_LATITUDE, z = ~pred_FLOOR,
              mode = "line", marker = list(size = 3, color = "chartreuse"), name = "Predicted")
```


```{r}
summary(real.pred.values$long_diff)
summary(real.pred.values$lat_diff)

hist(real.pred.values$long_diff)
hist(real.pred.values$lat_diff)
```

```{r warning=FALSE}
real.pred.values$PREDICTION = "Good"


for (i in 1:nrow(real.pred.values)) {
  if (real.pred.values$long_diff <= 14 & real.pred.values$lat_diff <= 14) {
    real.pred.values$PREDICTION = "VERYDOGGO"
    
  }
  
}

very_good = real.pred.values[which(real.pred.values$long_diff <= 3.5 & real.pred.values$lat_diff <= 3.5),]

plot_ly(very_good) %>%
  add_markers(x = ~real_LONGITUDE, y = ~real_LATITUDE, z = ~real_FLOOR,
              marker = list(size = 3, color = "black"), name = "Real") %>%
  add_markers(x = ~pred_LONGITUDE, y = ~pred_LATITUDE, z = ~pred_FLOOR,
              marker = list(size = 3, color = "chartreuse"), name = "Predicted") %>%
  layout(title = "Very Good Predictions")
```



