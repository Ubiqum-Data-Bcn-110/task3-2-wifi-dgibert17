### LOADING DATA ###
setwd("C:/Users/David/Google Drive/Github/task3-2-wifi-dgibert17")
load(file = "training.Rdata")
load(file = "validation.Rdata")
df.train.B0 = df.train %>%
filter(BUILDINGID == 0)
df.val.B0 = df.val %>%
filter(BUILDINGID == 0)
WAPnotDetected.val = as.character()
WAPnotDetected.tr = as.character()
# VALIDATION DF
for (i in 1:which(names(df.val.B0)=="WAP508")){
if (sum(df.val.B0[i]) == 100*nrow(df.val.B0)){
#cat("El", names(df.val[i]), "no ha sido detectado por ningun dispositivo en el validation\n")
WAPnotDetected.val = c(WAPnotDetected.val, names(df.val.B0[i]))
}
}
# TRAINING DF
for (i in 1:which(names(df.train.B0)=="WAP508")){
if (sum(df.train.B0[i]) == 100*nrow(df.train.B0)){
#cat("El", names(df.train[i]), "no ha sido detectado por ningun dispositivo en el training\n")
WAPnotDetected.tr = c(WAPnotDetected.tr, names(df.train.B0[i]))
}
}
WAPnotDetected.val
# which(names(df.val.B0) %in% WAPnotDetected.val) #Estos son los indices de los atributos que no son detectados en el validation set, por lo que quitaremos estos atributos del validation set.
df.val.B0 = df.val.B0[, -which(names(df.val.B0) %in% WAPnotDetected.val)]
df.train.B0 = df.train.B0[, -which(names(df.train.B0) %in% WAPnotDetected.tr)]
tr.in.val.idxs = which(names(df.train.B0) %in% names(df.val.B0))
df.train.B0 = df.train.B0[, tr.in.val.idxs]
val.in.tr.idxs = which(names(df.val.B0) %in% names(df.train.B0))
df.val.B0 = df.val.B0[, val.in.tr.idxs]
sum(names(df.train.B0) == names(df.val.B0))
# intersect(x = names(df.train), y = names(df.val))
df.train.B0$FLOOR = factor(df.train.B0$FLOOR)
df.train.B0$BUILDINGID = factor(df.train.B0$BUILDINGID)
df.val.B0$FLOOR = factor(df.val.B0$FLOOR)
df.val.B0$BUILDINGID = factor(df.val.B0$BUILDINGID)
str(df.train.B0[,c(140:149)])
str(df.val.B0[,c(140:149)])
floor.vec = createDataPartition(y = df.train.B0$FLOOR, times = 5, p = 0.1)
# Training data
floor.tr = df.train.B0[, c(1:last(grep(pattern = "WAP", names(df.train.B0))), which(names(df.train.B0) == "FLOOR"))]
floor.tr$FLOOR = factor(floor.tr$FLOOR)
#Data proportion from TRAINING (total) and VALIDATION (sample)
prop.table(table(df.train.B0$FLOOR))
prop.table(table(floor.tr$FLOOR))
knn.fit.floor = knn(train = floor.tr[,1:139], test = df.val.B0[,c(1:139)], cl = floor.tr[,140], k = 5)
confusionMatrix(data = factor(knn.fit.floor), reference = df.val.B0$FLOOR)
#Cuantos dispositivos se han clasificado mal por cada edificio.
abs(table(knn.fit.floor) - table(df.val.B0$FLOOR))
svm.fit.floor <- svm(formula = FLOOR ~ ., data = floor.tr)
svm.pred.floor <- predict(svm.fit.floor, newdata = df.val.B0[,c(1:139)])
confusionMatrix(data = svm.pred.floor, reference = df.val.B0$FLOOR)
# #Cuantos dispositivos se han clasificado mal por cada edificio.
abs(table(svm.pred.floor) - table(df.val.B0$FLOOR))
# bestmtry = tuneRF(x = floor.tr[,1:139], y = floor.tr$FLOOR, ntreeTry = 1000, plot = T,stepFactor = 1.5, improve = 1e-5)
rf.fit.floor = randomForest(x = floor.tr[,1:139], y = floor.tr$FLOOR, ntree = 200, mtry = 16)
rf.pred.floor = predict(rf.fit.floor, df.val.B0)
confusionMatrix(data = factor(rf.pred.floor), reference = df.val.B0$FLOOR)
#Cuantos dispositivos se han clasificado mal por cada Planta
abs(table(rf.pred.floor) - table(df.val.B0$FLOOR))
rm(list = ls())
library(dplyr)
library(Metrics)
library(rpart)
library(rpart.plot)
library(fastDummies)
library(ggplot2)
library(elasticnet)
library(randomForest)
library(lars)
library(caret)
library(AppliedPredictiveModeling)
library(RColorBrewer)
library(e1071)
library(pls)
library(plotly)
library(class)
### LOADING DATA ###
setwd("C:/Users/David/Google Drive/Github/task3-2-wifi-dgibert17")
load(file = "training.Rdata")
load(file = "validation.Rdata")
df.train.B0 = df.train %>%
filter(BUILDINGID == 1)
df.val.B0 = df.val %>%
filter(BUILDINGID == 1)
WAPnotDetected.val = as.character()
WAPnotDetected.tr = as.character()
# VALIDATION DF
for (i in 1:which(names(df.val.B0)=="WAP508")){
if (sum(df.val.B0[i]) == 100*nrow(df.val.B0)){
#cat("El", names(df.val[i]), "no ha sido detectado por ningun dispositivo en el validation\n")
WAPnotDetected.val = c(WAPnotDetected.val, names(df.val.B0[i]))
}
}
# TRAINING DF
for (i in 1:which(names(df.train.B0)=="WAP508")){
if (sum(df.train.B0[i]) == 100*nrow(df.train.B0)){
#cat("El", names(df.train[i]), "no ha sido detectado por ningun dispositivo en el training\n")
WAPnotDetected.tr = c(WAPnotDetected.tr, names(df.train.B0[i]))
}
}
WAPnotDetected.val
which(names(df.val.B0) %in% WAPnotDetected.val) #Estos son los indices de los atributos que no son detectados en el validation set, por lo que quitaremos estos atributos del validation set.
df.val.B0 = df.val.B0[, -which(names(df.val.B0) %in% WAPnotDetected.val)]
df.train.B0 = df.train.B0[, -which(names(df.train.B0) %in% WAPnotDetected.tr)]
tr.in.val.idxs = which(names(df.train.B0) %in% names(df.val.B0))
df.train.B0 = df.train.B0[, tr.in.val.idxs]
val.in.tr.idxs = which(names(df.val.B0) %in% names(df.train.B0))
df.val.B0 = df.val.B0[, val.in.tr.idxs]
sum(names(df.train.B0) == names(df.val.B0))
# intersect(x = names(df.train), y = names(df.val))
df.train.B0$FLOOR = factor(df.train.B0$FLOOR)
df.train.B0$BUILDINGID = factor(df.train.B0$BUILDINGID)
df.val.B0$FLOOR = factor(df.val.B0$FLOOR)
df.val.B0$BUILDINGID = factor(df.val.B0$BUILDINGID)
floor.vec = createDataPartition(y = df.train.B0$FLOOR, times = 5, p = 0.3)
# Training data
floor.tr = df.train.B0[, c(1:last(grep(pattern = "WAP", names(df.train.B0))), which(names(df.train.B0) == "FLOOR"))]
floor.tr$FLOOR = factor(floor.tr$FLOOR)
#Proporcion de datos en TRAIN y SAMPLE
prop.table(table(df.train.B0$FLOOR))
prop.table(table(floor.tr$FLOOR))
knn.fit.floor = knn(train = floor.tr[,1:146], test = df.val.B0[,c(1:146)], cl = floor.tr[,147], k = 5)
confusionMatrix(data = factor(knn.fit.floor), reference = df.val.B0$FLOOR)
#Cuantos dispositivos se han clasificado mal por cada edificio.
abs(table(knn.fit.floor) - table(df.val.B0$FLOOR))
svm.fit.floor <- svm(formula = FLOOR ~ ., data = floor.tr)
svm.pred.floor <- predict(svm.fit.floor, newdata = df.val.B0[,c(1:146)])
confusionMatrix(data = svm.pred.floor, reference = df.val.B0$FLOOR)
# #Cuantos dispositivos se han clasificado mal por cada edificio.
abs(table(svm.pred.floor) - table(df.val.B0$FLOOR))
# bestmtry = tuneRF(x = floor.tr[,1:146], y = floor.tr$FLOOR, ntreeTry = 400, plot = T,stepFactor = 1.5, improve = 1e-5)
rf.fit.floor = randomForest(x = floor.tr[,1:146], y = floor.tr$FLOOR, ntree = 400, mtry = 18)
rf.pred.floor = predict(rf.fit.floor, df.val.B0)
confusionMatrix(data = factor(rf.pred.floor), reference = df.val.B0$FLOOR)
#Cuantos dispositivos se han clasificado mal por cada Planta
abs(table(rf.pred.floor) - table(df.val.B0$FLOOR))
rm(list = ls())
library(dplyr)
library(Metrics)
library(rpart)
library(rpart.plot)
library(fastDummies)
library(ggplot2)
library(elasticnet)
library(randomForest)
library(lars)
library(caret)
library(AppliedPredictiveModeling)
library(RColorBrewer)
library(e1071)
library(pls)
library(plotly)
library(class)
### LOADING DATA ###
setwd("C:/Users/David/Google Drive/Github/task3-2-wifi-dgibert17")
load(file = "training.Rdata")
load(file = "validation.Rdata")
df.train.B0 = df.train %>%
filter(BUILDINGID == 1)
df.val.B0 = df.val %>%
filter(BUILDINGID == 1)
WAPnotDetected.val = as.character()
WAPnotDetected.tr = as.character()
# VALIDATION DF
for (i in 1:which(names(df.val.B0)=="WAP508")){
if (sum(df.val.B0[i]) == 100*nrow(df.val.B0)){
#cat("El", names(df.val[i]), "no ha sido detectado por ningun dispositivo en el validation\n")
WAPnotDetected.val = c(WAPnotDetected.val, names(df.val.B0[i]))
}
}
# TRAINING DF
for (i in 1:which(names(df.train.B0)=="WAP508")){
if (sum(df.train.B0[i]) == 100*nrow(df.train.B0)){
#cat("El", names(df.train[i]), "no ha sido detectado por ningun dispositivo en el training\n")
WAPnotDetected.tr = c(WAPnotDetected.tr, names(df.train.B0[i]))
}
}
WAPnotDetected.val
which(names(df.val.B0) %in% WAPnotDetected.val) #Estos son los indices de los atributos que no son detectados en el validation set, por lo que quitaremos estos atributos del validation set.
df.val.B0 = df.val.B0[, -which(names(df.val.B0) %in% WAPnotDetected.val)]
df.train.B0 = df.train.B0[, -which(names(df.train.B0) %in% WAPnotDetected.tr)]
tr.in.val.idxs = which(names(df.train.B0) %in% names(df.val.B0))
df.train.B0 = df.train.B0[, tr.in.val.idxs]
val.in.tr.idxs = which(names(df.val.B0) %in% names(df.train.B0))
df.val.B0 = df.val.B0[, val.in.tr.idxs]
sum(names(df.train.B0) == names(df.val.B0))
# intersect(x = names(df.train), y = names(df.val))
df.train.B0$FLOOR = factor(df.train.B0$FLOOR)
df.train.B0$BUILDINGID = factor(df.train.B0$BUILDINGID)
df.val.B0$FLOOR = factor(df.val.B0$FLOOR)
df.val.B0$BUILDINGID = factor(df.val.B0$BUILDINGID)
floor.vec = createDataPartition(y = df.train.B0$FLOOR, times = 5, p = 0.3)
# Training data
floor.tr = df.train.B0[, c(1:last(grep(pattern = "WAP", names(df.train.B0))), which(names(df.train.B0) == "FLOOR"))]
floor.tr$FLOOR = factor(floor.tr$FLOOR)
#Proporcion de datos en TRAIN y SAMPLE
prop.table(table(df.train.B0$FLOOR))
prop.table(table(floor.tr$FLOOR))
knn.fit.floor = knn(train = floor.tr[,1:146], test = df.val.B0[,c(1:146)], cl = floor.tr[,147], k = 5)
confusionMatrix(data = factor(knn.fit.floor), reference = df.val.B0$FLOOR)
#Cuantos dispositivos se han clasificado mal por cada edificio.
abs(table(knn.fit.floor) - table(df.val.B0$FLOOR))
svm.fit.floor <- svm(formula = FLOOR ~ ., data = floor.tr)
svm.pred.floor <- predict(svm.fit.floor, newdata = df.val.B0[,c(1:146)])
confusionMatrix(data = svm.pred.floor, reference = df.val.B0$FLOOR)
# #Cuantos dispositivos se han clasificado mal por cada edificio.
abs(table(svm.pred.floor) - table(df.val.B0$FLOOR))
# bestmtry = tuneRF(x = floor.tr[,1:146], y = floor.tr$FLOOR, ntreeTry = 400, plot = T,stepFactor = 1.5, improve = 1e-5)
rf.fit.floor = randomForest(x = floor.tr[,1:146], y = floor.tr$FLOOR, ntree = 400, mtry = 18)
rf.pred.floor = predict(rf.fit.floor, df.val.B0)
confusionMatrix(data = factor(rf.pred.floor), reference = df.val.B0$FLOOR)
#Cuantos dispositivos se han clasificado mal por cada Planta
abs(table(rf.pred.floor) - table(df.val.B0$FLOOR))
df.val.B0$FLOOR = rf.pred.floor
str(df.val.B0$FLOOR)
pacman::p_load(fastDummies)
df.val.B0.dumm = dummy_cols(.data = df.val.B0, select_columns = "FLOOR", remove_first_dummy = FALSE)
df.val.B0.dumm = select(.data = df.val.B0.dumm, -FLOOR)
df.train.B0.dumm = dummy_cols(.data = df.train.B0, select_columns = "FLOOR", remove_first_dummy = FALSE)
df.train.B0.dumm = select(.data = df.train.B0.dumm, -FLOOR)
svm.fit.lat <- svm(formula = LATITUDE ~ ., data = df.train.B0.dumm[,c(1:146, 148, 156:159)])
svm.pred.lat <- predict(svm.fit.lat, newdata = df.val.B0.dumm[,c(1:146, 156:159)])
rmse(actual = df.val.B0.dumm$LATITUDE, predicted = svm.pred.lat)
mae(actual = df.val.B0.dumm$LATITUDE, predicted = svm.pred.lat)
lm.fit.lat <- lm(formula = LATITUDE ~ ., data = df.train.B0.dumm[,c(1:146, 148, 156:159)])
lm.pred.lat <- predict(lm.fit.lat, newdata = df.val.B0.dumm[,c(1:146, 156:159)])
rmse(actual = df.val.B0.dumm$LATITUDE, predicted = lm.pred.lat)
mae(actual = df.val.B0.dumm$LATITUDE, predicted = lm.pred.lat)
# bestmtry = tuneRF(x = df.train.B0.dumm[,c(1:146, 156:159)], y = df.train.B0.dumm$LATITUDE, ntreeTry = 400, plot = T,stepFactor = 1.5, improve = 1e-5)
rf.fit.lat = randomForest(LATITUDE ~., df.train.B0.dumm[,c(1:146, 148, 156:159)], ntree = 400, mtry = 34)
rf.pred.lat = predict(rf.fit.lat, df.val.B0.dumm[,c(1:146, 156:159)])
rmse(actual = df.val.B0.dumm$LATITUDE, predicted = rf.pred.lat)
mae(actual = df.val.B0.dumm$LATITUDE, predicted = rf.pred.lat)
pacman::p_load(FNN)
knn.fit.lat = knn.reg(train = df.train.B0.dumm[,c(1:146, 156:159)], test = df.val.B0.dumm[,c(1:146, 156:159)],
y = df.train.B0.dumm[, 148], k = 5,
algorithm = c("kd_tree", "cover_tree", "brute"))
rmse(actual = df.val.B0.dumm$LATITUDE, predicted = knn.fit.lat$pred)
mae(actual = df.val.B0.dumm$LATITUDE, predicted = knn.fit.lat$pred)
df.val.B0.dumm$LATITUDE = rf.pred.lat
df.val.B0.dumm$LATITUDE = rf.pred.lat
svm.fit.long <- svm(formula = LONGITUDE ~ ., data = df.train.B0.dumm[,c(1:146, 147, 148, 156:159)])
svm.pred.long <- predict(svm.fit.long, newdata = df.val.B0.dumm[,c(1:146, 148, 156:159)])
rmse(actual = df.val.B0.dumm$LONGITUDE, predicted = svm.pred.long)
mae(actual = df.val.B0.dumm$LONGITUDE, predicted = svm.pred.long)
lm.fit.long <- lm(formula = LONGITUDE ~ ., data = df.train.B0.dumm[,c(1:146, 147, 148, 156:159)])
lm.pred.long <- predict(lm.fit.long, newdata = df.val.B0.dumm[,c(1:146, 148, 156:159)])
rmse(actual = df.val.B0.dumm$LONGITUDE, predicted = lm.pred.long)
mae(actual = df.val.B0.dumm$LONGITUDE, predicted = lm.pred.long)
# bestmtry = tuneRF(x = df.train.B0.dumm[,c(1:146, 148, 156:159)], y = df.train.B0.dumm$LONGITUDE, ntreeTry = 400, plot = T,stepFactor = 1.5, improve = 1e-5)
rf.fit.long = randomForest(LONGITUDE ~., df.train.B0.dumm[,c(1:146, 147, 148, 156:159)], ntree = 400, mtry = 50)
rf.pred.long = predict(rf.fit.long, df.val.B0.dumm[,c(1:146, 148, 156:159)])
rmse(actual = df.val.B0.dumm$LONGITUDE, predicted = rf.pred.long)
mae(actual = df.val.B0.dumm$LONGITUDE, predicted = rf.pred.long)
pacman::p_load(FNN)
knn.fit.long = knn.reg(train = df.train.B0.dumm[,c(1:146, 148, 156:159)], test = df.val.B0.dumm[,c(1:146, 148, 156:159)],
y = df.train.B0.dumm[,147], k = 5,
algorithm = c("kd_tree", "cover_tree", "brute"))
rmse(actual = df.val.B0.dumm$LONGITUDE, predicted = knn.fit.long$pred)
mae(actual = df.val.B0.dumm$LONGITUDE, predicted = knn.fit.long$pred)
rm(list = ls())
library(dplyr)
library(Metrics)
library(rpart)
library(rpart.plot)
library(fastDummies)
library(ggplot2)
library(elasticnet)
library(randomForest)
library(lars)
library(caret)
library(AppliedPredictiveModeling)
library(RColorBrewer)
library(e1071)
library(pls)
library(plotly)
library(class)
### LOADING DATA ###
setwd("C:/Users/David/Google Drive/Github/task3-2-wifi-dgibert17")
load(file = "training.Rdata")
load(file = "validation.Rdata")
df.train.B0 = df.train %>%
filter(BUILDINGID == 2)
df.val.B0 = df.val %>%
filter(BUILDINGID == 2)
WAPnotDetected.val = as.character()
WAPnotDetected.tr = as.character()
# VALIDATION DF
for (i in 1:which(names(df.val.B0)=="WAP508")){
if (sum(df.val.B0[i]) == 100*nrow(df.val.B0)){
#cat("El", names(df.val[i]), "no ha sido detectado por ningun dispositivo en el validation\n")
WAPnotDetected.val = c(WAPnotDetected.val, names(df.val.B0[i]))
}
}
# TRAINING DF
for (i in 1:which(names(df.train.B0)=="WAP508")){
if (sum(df.train.B0[i]) == 100*nrow(df.train.B0)){
#cat("El", names(df.train[i]), "no ha sido detectado por ningun dispositivo en el training\n")
WAPnotDetected.tr = c(WAPnotDetected.tr, names(df.train.B0[i]))
}
}
WAPnotDetected.val
# which(names(df.val.B0) %in% WAPnotDetected.val) #Estos son los indices de los atributos que no son detectados en el validation set, por lo que quitaremos estos atributos del validation set.
df.val.B0 = df.val.B0[, -which(names(df.val.B0) %in% WAPnotDetected.val)]
df.train.B0 = df.train.B0[, -which(names(df.train.B0) %in% WAPnotDetected.tr)]
rm(list = ls())
library(dplyr)
library(Metrics)
library(rpart)
library(rpart.plot)
library(fastDummies)
library(ggplot2)
library(elasticnet)
library(randomForest)
library(lars)
library(caret)
library(AppliedPredictiveModeling)
library(RColorBrewer)
library(e1071)
library(pls)
library(plotly)
library(class)
### LOADING DATA ###
setwd("C:/Users/David/Google Drive/Github/task3-2-wifi-dgibert17")
load(file = "training.Rdata")
load(file = "validation.Rdata")
df.train.B0 = df.train %>%
filter(BUILDINGID == 2)
df.val.B0 = df.val %>%
filter(BUILDINGID == 2)
WAPnotDetected.val = as.character()
WAPnotDetected.tr = as.character()
# VALIDATION DF
for (i in 1:which(names(df.val.B0)=="WAP508")){
if (sum(df.val.B0[i]) == 100*nrow(df.val.B0)){
#cat("El", names(df.val[i]), "no ha sido detectado por ningun dispositivo en el validation\n")
WAPnotDetected.val = c(WAPnotDetected.val, names(df.val.B0[i]))
}
}
# TRAINING DF
for (i in 1:which(names(df.train.B0)=="WAP508")){
if (sum(df.train.B0[i]) == 100*nrow(df.train.B0)){
#cat("El", names(df.train[i]), "no ha sido detectado por ningun dispositivo en el training\n")
WAPnotDetected.tr = c(WAPnotDetected.tr, names(df.train.B0[i]))
}
}
WAPnotDetected.val
# which(names(df.val.B0) %in% WAPnotDetected.val) #Estos son los indices de los atributos que no son detectados en el validation set, por lo que quitaremos estos atributos del validation set.
df.val.B0 = df.val.B0[, -which(names(df.val.B0) %in% WAPnotDetected.val)]
df.train.B0 = df.train.B0[, -which(names(df.train.B0) %in% WAPnotDetected.tr)]
tr.in.val.idxs = which(names(df.train.B0) %in% names(df.val.B0))
df.train.B0 = df.train.B0[, tr.in.val.idxs]
val.in.tr.idxs = which(names(df.val.B0) %in% names(df.train.B0))
df.val.B0 = df.val.B0[, val.in.tr.idxs]
sum(names(df.train.B0) == names(df.val.B0))
# intersect(x = names(df.train), y = names(df.val))
df.train.B0$FLOOR = factor(df.train.B0$FLOOR)
df.train.B0$BUILDINGID = factor(df.train.B0$BUILDINGID)
df.val.B0$FLOOR = factor(df.val.B0$FLOOR)
df.val.B0$BUILDINGID = factor(df.val.B0$BUILDINGID)
floor.vec = createDataPartition(y = df.train.B0$FLOOR, times = 5, p = 0.1)
# Training data
floor.tr = df.train.B0[, c(1:last(grep(pattern = "WAP", names(df.train.B0))), which(names(df.train.B0) == "FLOOR"))]
floor.tr$FLOOR = factor(floor.tr$FLOOR)
#Proporcion de datos en TRAIN y SAMPLE
prop.table(table(df.train.B0$FLOOR))
prop.table(table(floor.tr$FLOOR))
knn.fit.floor = knn(train = floor.tr[,1:106], test = df.val.B0[,c(1:106)], cl = floor.tr[,107], k = 5)
confusionMatrix(data = factor(knn.fit.floor), reference = df.val.B0$FLOOR)
#Cuantos dispositivos se han clasificado mal por cada edificio.
abs(table(knn.fit.floor) - table(df.val.B0$FLOOR))
svm.fit.floor <- svm(formula = FLOOR ~ ., data = floor.tr)
svm.pred.floor <- predict(svm.fit.floor, newdata = df.val.B0[,c(1:106)])
confusionMatrix(data = svm.pred.floor, reference = df.val.B0$FLOOR)
# #Cuantos dispositivos se han clasificado mal por cada edificio.
abs(table(svm.pred.floor) - table(df.val.B0$FLOOR))
# bestmtry = tuneRF(x = floor.tr[,1:106], y = floor.tr$FLOOR, ntreeTry = 400, plot = T,stepFactor = 1.5, improve = 1e-5)
rf.fit.floor = randomForest(x = floor.tr[,1:106], y = floor.tr$FLOOR, ntree = 400, mtry = 15)
rf.pred.floor = predict(rf.fit.floor, df.val.B0)
confusionMatrix(data = factor(rf.pred.floor), reference = df.val.B0$FLOOR)
#Cuantos dispositivos se han clasificado mal por cada Planta
abs(table(rf.pred.floor) - table(df.val.B0$FLOOR))
knn.fit.floor = knn(train = floor.tr[,1:106], test = df.val.B0[,c(1:106)], cl = floor.tr[,107], k = 5)
confusionMatrix(data = factor(knn.fit.floor), reference = df.val.B0$FLOOR)
#Cuantos dispositivos se han clasificado mal por cada edificio.
abs(table(knn.fit.floor) - table(df.val.B0$FLOOR))
rm(list = ls())
library(dplyr)
library(Metrics)
library(rpart)
library(rpart.plot)
library(fastDummies)
library(ggplot2)
library(elasticnet)
library(randomForest)
library(lars)
library(caret)
library(AppliedPredictiveModeling)
library(RColorBrewer)
library(e1071)
library(pls)
library(plotly)
library(class)
### LOADING DATA ###
setwd("C:/Users/David/Google Drive/Github/task3-2-wifi-dgibert17")
load(file = "training.Rdata")
load(file = "validation.Rdata")
df.train.B0 = df.train %>%
filter(BUILDINGID == 2)
df.val.B0 = df.val %>%
filter(BUILDINGID == 2)
WAPnotDetected.val = as.character()
WAPnotDetected.tr = as.character()
# VALIDATION DF
for (i in 1:which(names(df.val.B0)=="WAP508")){
if (sum(df.val.B0[i]) == 100*nrow(df.val.B0)){
#cat("El", names(df.val[i]), "no ha sido detectado por ningun dispositivo en el validation\n")
WAPnotDetected.val = c(WAPnotDetected.val, names(df.val.B0[i]))
}
}
# TRAINING DF
for (i in 1:which(names(df.train.B0)=="WAP508")){
if (sum(df.train.B0[i]) == 100*nrow(df.train.B0)){
#cat("El", names(df.train[i]), "no ha sido detectado por ningun dispositivo en el training\n")
WAPnotDetected.tr = c(WAPnotDetected.tr, names(df.train.B0[i]))
}
}
WAPnotDetected.val
# which(names(df.val.B0) %in% WAPnotDetected.val) #Estos son los indices de los atributos que no son detectados en el validation set, por lo que quitaremos estos atributos del validation set.
df.val.B0 = df.val.B0[, -which(names(df.val.B0) %in% WAPnotDetected.val)]
df.train.B0 = df.train.B0[, -which(names(df.train.B0) %in% WAPnotDetected.tr)]
tr.in.val.idxs = which(names(df.train.B0) %in% names(df.val.B0))
df.train.B0 = df.train.B0[, tr.in.val.idxs]
val.in.tr.idxs = which(names(df.val.B0) %in% names(df.train.B0))
df.val.B0 = df.val.B0[, val.in.tr.idxs]
sum(names(df.train.B0) == names(df.val.B0))
# intersect(x = names(df.train), y = names(df.val))
df.train.B0$FLOOR = factor(df.train.B0$FLOOR)
df.train.B0$BUILDINGID = factor(df.train.B0$BUILDINGID)
df.val.B0$FLOOR = factor(df.val.B0$FLOOR)
df.val.B0$BUILDINGID = factor(df.val.B0$BUILDINGID)
floor.vec = createDataPartition(y = df.train.B0$FLOOR, times = 5, p = 0.1)
# Training data
floor.tr = df.train.B0[, c(1:last(grep(pattern = "WAP", names(df.train.B0))), which(names(df.train.B0) == "FLOOR"))]
floor.tr$FLOOR = factor(floor.tr$FLOOR)
#Proporcion de datos en TRAIN y SAMPLE
prop.table(table(df.train.B0$FLOOR))
prop.table(table(floor.tr$FLOOR))
knn.fit.floor = knn(train = floor.tr[,1:106], test = df.val.B0[,c(1:106)], cl = floor.tr[,107], k = 5)
confusionMatrix(data = factor(knn.fit.floor), reference = df.val.B0$FLOOR)
#Cuantos dispositivos se han clasificado mal por cada edificio.
abs(table(knn.fit.floor) - table(df.val.B0$FLOOR))
svm.fit.floor <- svm(formula = FLOOR ~ ., data = floor.tr)
svm.pred.floor <- predict(svm.fit.floor, newdata = df.val.B0[,c(1:106)])
confusionMatrix(data = svm.pred.floor, reference = df.val.B0$FLOOR)
# #Cuantos dispositivos se han clasificado mal por cada edificio.
abs(table(svm.pred.floor) - table(df.val.B0$FLOOR))
# bestmtry = tuneRF(x = floor.tr[,1:106], y = floor.tr$FLOOR, ntreeTry = 400, plot = T,stepFactor = 1.5, improve = 1e-5)
rf.fit.floor = randomForest(x = floor.tr[,1:106], y = floor.tr$FLOOR, ntree = 400, mtry = 15)
rf.pred.floor = predict(rf.fit.floor, df.val.B0)
confusionMatrix(data = factor(rf.pred.floor), reference = df.val.B0$FLOOR)
#Cuantos dispositivos se han clasificado mal por cada Planta
abs(table(rf.pred.floor) - table(df.val.B0$FLOOR))
df.val.B0$FLOOR = rf.pred.floor
str(df.val.B0$FLOOR)
pacman::p_load(fastDummies)
df.val.B0.dumm = dummy_cols(.data = df.val.B0, select_columns = "FLOOR", remove_first_dummy = FALSE)
df.val.B0.dumm = select(.data = df.val.B0.dumm, -FLOOR)
df.train.B0.dumm = dummy_cols(.data = df.train.B0, select_columns = "FLOOR", remove_first_dummy = FALSE)
df.train.B0.dumm = select(.data = df.train.B0.dumm, -FLOOR)
svm.fit.lat <- svm(formula = LATITUDE ~ ., data = df.train.B0.dumm[,c(1:106, 108, 116:120)])
svm.pred.lat <- predict(svm.fit.lat, newdata = df.val.B0.dumm[,c(1:106, 116:120)])
rmse(actual = df.val.B0.dumm$LATITUDE, predicted = svm.pred.lat)
mae(actual = df.val.B0.dumm$LATITUDE, predicted = svm.pred.lat)
lm.fit.lat <- lm(formula = LATITUDE ~ ., data = df.train.B0.dumm[,c(1:106, 108, 116:120)])
lm.pred.lat <- predict(lm.fit.lat, newdata = df.val.B0.dumm[,c(1:106, 116:120)])
rmse(actual = df.val.B0.dumm$LATITUDE, predicted = lm.pred.lat)
mae(actual = df.val.B0.dumm$LATITUDE, predicted = lm.pred.lat)
# bestmtry = tuneRF(x = df.train.B0.dumm[,c(1:106, 116:120)], y = df.train.B0.dumm$LATITUDE, ntreeTry = 400, plot = T,stepFactor = 1.5, improve = 1e-5)
rf.fit.lat = randomForest(LATITUDE ~., df.train.B0.dumm[,c(1:106, 108, 116:120)], ntree = 150, mtry = 37)
rf.pred.lat = predict(rf.fit.lat, df.val.B0.dumm[,c(1:106, 116:120)])
rmse(actual = df.val.B0.dumm$LATITUDE, predicted = rf.pred.lat)
mae(actual = df.val.B0.dumm$LATITUDE, predicted = rf.pred.lat)
pacman::p_load(FNN)
knn.fit.lat = knn.reg(train = df.train.B0.dumm[,c(1:106, 116:120)], test = df.val.B0.dumm[,c(1:106, 116:120)],
y = df.train.B0.dumm[, 108], k = 5,
algorithm = c("kd_tree", "cover_tree", "brute"))
rmse(actual = df.val.B0.dumm$LATITUDE, predicted = knn.fit.lat$pred)
mae(actual = df.val.B0.dumm$LATITUDE, predicted = knn.fit.lat$pred)
df.val.B0.dumm$LATITUDE = rf.pred.lat
svm.fit.long <- svm(formula = LONGITUDE ~ ., data = df.train.B0.dumm[,c(1:106, 107, 108, 116:120)])
svm.pred.long <- predict(svm.fit.long, newdata = df.val.B0.dumm[,c(1:106, 108, 116:120)])
rmse(actual = df.val.B0.dumm$LONGITUDE, predicted = svm.pred.long)
mae(actual = df.val.B0.dumm$LONGITUDE, predicted = svm.pred.long)
lm.fit.long <- lm(formula = LONGITUDE ~ ., data = df.train.B0.dumm[,c(1:106, 107, 108, 116:120)])
lm.pred.long <- predict(lm.fit.long, newdata = df.val.B0.dumm[,c(1:106, 108, 116:120)])
rmse(actual = df.val.B0.dumm$LONGITUDE, predicted = lm.pred.long)
mae(actual = df.val.B0.dumm$LONGITUDE, predicted = lm.pred.long)
# bestmtry = tuneRF(x = df.train.B0.dumm[,c(1:106, 108, 116:120)], y = df.train.B0.dumm$LONGITUDE, ntreeTry = 150, plot = T,stepFactor = 1.5, improve = 1e-5)
rf.fit.long = randomForest(LONGITUDE ~., df.train.B0.dumm[,c(1:106, 107, 108, 116:120)], ntree = 150, mtry = 82)
rf.pred.long = predict(rf.fit.long, df.val.B0.dumm[,c(1:106, 108, 116:120)])
rmse(actual = df.val.B0.dumm$LONGITUDE, predicted = rf.pred.long)
mae(actual = df.val.B0.dumm$LONGITUDE, predicted = rf.pred.long)
pacman::p_load(FNN)
knn.fit.long = knn.reg(train = df.train.B0.dumm[,c(1:106, 108, 116:120)], test = df.val.B0.dumm[,c(1:106, 108, 116:120)],
y = df.train.B0.dumm[,107], k = 5,
algorithm = c("kd_tree", "cover_tree", "brute"))
rmse(actual = df.val.B0.dumm$LONGITUDE, predicted = knn.fit.long$pred)
mae(actual = df.val.B0.dumm$LONGITUDE, predicted = knn.fit.long$pred)
