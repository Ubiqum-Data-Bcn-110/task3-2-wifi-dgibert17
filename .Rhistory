library(dplyr)
```
```{r}
### LOADING DATA ###
df.train = read.csv("C:/Users/David/Google Drive/Github/task3-2-wifi-dgibert17/training.csv")
# df.val = read.csv("C:/Users/David/Google Drive/Github/task3-2-wifi-dgibert17/validation.csv")
```
rm(list = ls())
### LIBRARIES ###
library(dplyr)
### LOADING DATA ###
df.train = read.csv("C:/Users/David/Google Drive/Github/task3-2-wifi-dgibert17/training.csv")
# df.val = read.csv("C:/Users/David/Google Drive/Github/task3-2-wifi-dgibert17/validation.csv")
rm(list = ls())
### LIBRARIES ###
library(dplyr)
### LOADING DATA ###
setwd("C:/Users/David/Google Drive/Github/task3-2-wifi-dgibert17")
df.train = read.csv("training.csv")
# df.val = read.csv("validation.csv")
str(df.train)
str(df.train, list.len = ncol(df.train))
df.str = str(df.train, list.len = ncol(df.train))
tail(str(df.train))[:10,]
tail(str(df.train))
str(df.train)[520:529,]
tail(str(df.train), 10)
df.train[520:nrow(df.train), ]
df.train[, 520:ncol(df.train)]
str(df.train[, 520:ncol(df.train)])
str(df.train[, 521:ncol(df.train)])
sum(is.na(df.train))
str(df.train[, 521:ncol(df.train)])
head(df.train)
summary(df.train$WAP001)
summary(as.factor(df.train$WAP001))
table(df.train$WAP001)
names(df.train)
library(corrplot)
head(df.train[, 521:529])
rm(list = ls())
### LIBRARIES ###
library(dplyr)
library(corrplot)
### LOADING DATA ###
setwd("C:/Users/David/Google Drive/Github/task3-2-wifi-dgibert17")
df.train = read.csv("training.csv")
# df.val = read.csv("validation.csv")
str(df.train[, 521:ncol(df.train)])
head(df.train[, 521:529])
head(df.train[, 521:523])
head(df.train[, 521:529])
sum(df.train$WAP001)
100*19937
rm(list = ls())
### LIBRARIES ###
library(dplyr)
library(Metrics)
library(rpart)
library(doParallel)
library(rpart.plot)
library(fastDummies)
library(ggplot2)
library(elasticnet)
library(lars)
library(caret)
library(AppliedPredictiveModeling)
library(e1071)
library(pls)
library(plotly)
## LOADING DATA
setwd("C:/Users/David/Google Drive/Github/task3-2-wifi-dgibert17")
load(file = "training.Rdata")
load(file = "validation.Rdata")
# cl <- makeCluster(3)
# doParallel:::registerDoParallel(cl)
bf.vec = createDataPartition(y = df.train$BF, p = 0.05)
# Training data
# bf.tr = df.train[bf.vec$Resample1, c(1:last(grep(pattern = "WAP", names(df.train))), which(names(df.train) == "BF"))]
bf.tr = df.train[bf.vec$Resample1, c(1:last(grep(pattern = "WAP", names(df.train))), 316,322)]
# Test data
# VALIDATION DF
# Tabla de proporciones para ver que la proporcion de observaciones por BUILDING es la misma en el DF original que en el bf.tr
prop.table(table(df.train$BF))
prop.table(table(bf.tr$BF))
ctrl = trainControl(method = "repeatedcv",
number = 3,
repeats = 10)
knn.fit.bf = train(BF~.,
method = "knn",
trControl = ctrl,
data = bf.tr)
knn.fit.bf
knn.pred.bf = predict(knn.fit.bf, df.val)
accuracy(actual = df.val$BF, predicted = knn.pred.bf)
#Cuantas observaciones hay predichas por edificio y cuantas hay en realidad en total.
table(knn.pred.bf)
table(df.val$BF)
#Cuantos dispositivos se han clasificado mal por cada edificio.
abs(table(knn.pred.bf) - table(df.val$BF))
svmLin.fit.bf <- train(BF~.,
method = "svmLinear",
trControl = ctrl,
data = bf.tr
)
svmLin.fit.bf
svmLin.pred.bf = predict(svmLin.fit.bf, df.val)
accuracy(actual = df.val$BF, predicted = svmLin.pred.bf)
svmRad.fit.bf <- train(BF~.,
method = "svmRadial",
trControl = ctrl,
data = bf.tr
)
svmRad.fit.bf
svmRad.pred.bf = predict(svmRad.fit.bf, df.val)
accuracy(actual = df.val$BF, predicted = svmRad.pred.bf)
rf.fit.bf = train(BF~.,
data = bf.tr,
method = "rf",
ntree = 60,
trControl = ctrl,
allowParallel = TRUE
)
rf.pred.bf = predict(rf.fit.bf, df.val)
accuracy(actual = df.val$BF, predicted = rf.pred.bf)
str(bf.tr$BUILDINGID)
summary(rf.fit.bf)
rf.fit.bf
rm(list = ls())
### LIBRARIES ###
library(dplyr)
library(Metrics)
library(rpart)
library(doParallel)
library(rpart.plot)
library(fastDummies)
library(ggplot2)
library(elasticnet)
library(lars)
library(caret)
library(AppliedPredictiveModeling)
library(e1071)
library(pls)
library(plotly)
## LOADING DATA
setwd("C:/Users/David/Google Drive/Github/task3-2-wifi-dgibert17")
load(file = "training.Rdata")
load(file = "validation.Rdata")
# cl <- makeCluster(3)
# doParallel:::registerDoParallel(cl)
bf.vec = createDataPartition(y = df.train$BF, p = 0.05)
# Training data
# bf.tr = df.train[bf.vec$Resample1, c(1:last(grep(pattern = "WAP", names(df.train))), which(names(df.train) == "BF"))]
bf.tr = df.train[bf.vec$Resample1, c(1:last(grep(pattern = "WAP", names(df.train))), 316,322)]
bf.tr$BUILDINGID = factor(bf.tr$BUILDINGID)
# Test data
# VALIDATION DF
# Tabla de proporciones para ver que la proporcion de observaciones por BUILDING es la misma en el DF original que en el bf.tr
prop.table(table(df.train$BF))
prop.table(table(bf.tr$BF))
ctrl = trainControl(method = "repeatedcv",
number = 3,
repeats = 10)
knn.fit.bf = train(BF~.,
method = "knn",
trControl = ctrl,
data = bf.tr)
knn.fit.bf
knn.pred.bf = predict(knn.fit.bf, df.val)
str(df.val$BUILDINGID)
rm(list = ls())
### LIBRARIES ###
library(dplyr)
library(Metrics)
library(rpart)
library(doParallel)
library(rpart.plot)
library(fastDummies)
library(ggplot2)
library(elasticnet)
library(lars)
library(caret)
library(AppliedPredictiveModeling)
library(e1071)
library(pls)
library(plotly)
## LOADING DATA
setwd("C:/Users/David/Google Drive/Github/task3-2-wifi-dgibert17")
load(file = "training.Rdata")
load(file = "validation.Rdata")
# cl <- makeCluster(3)
# doParallel:::registerDoParallel(cl)
bf.vec = createDataPartition(y = df.train$BF, p = 0.05)
# Training data
# bf.tr = df.train[bf.vec$Resample1, c(1:last(grep(pattern = "WAP", names(df.train))), which(names(df.train) == "BF"))]
bf.tr = df.train[bf.vec$Resample1, c(1:last(grep(pattern = "WAP", names(df.train))), 316,322)]
bf.tr$BUILDINGID = factor(bf.tr$BUILDINGID)
df.val$BUILDINGID = factor(df.val$BUILDINGID)
# Test data
# VALIDATION DF
# Tabla de proporciones para ver que la proporcion de observaciones por BUILDING es la misma en el DF original que en el bf.tr
prop.table(table(df.train$BF))
prop.table(table(bf.tr$BF))
ctrl = trainControl(method = "repeatedcv",
number = 3,
repeats = 10)
knn.fit.bf = train(BF~.,
method = "knn",
trControl = ctrl,
data = bf.tr)
knn.fit.bf
knn.pred.bf = predict(knn.fit.bf, df.val)
accuracy(actual = df.val$BF, predicted = knn.pred.bf)
#Cuantas observaciones hay predichas por edificio y cuantas hay en realidad en total.
table(knn.pred.bf)
table(df.val$BF)
#Cuantos dispositivos se han clasificado mal por cada edificio.
abs(table(knn.pred.bf) - table(df.val$BF))
svmLin.fit.bf <- train(BF~.,
method = "svmLinear",
trControl = ctrl,
data = bf.tr
)
svmLin.fit.bf
svmLin.pred.bf = predict(svmLin.fit.bf, df.val)
accuracy(actual = df.val$BF, predicted = svmLin.pred.bf)
svmRad.fit.bf <- train(BF~.,
method = "svmRadial",
trControl = ctrl,
data = bf.tr
)
svmRad.fit.bf
svmRad.pred.bf = predict(svmRad.fit.bf, df.val)
accuracy(actual = df.val$BF, predicted = svmRad.pred.bf)
rf.fit.bf = train(BF~.,
data = bf.tr,
method = "rf",
ntree = 40,
trControl = ctrl,
allowParallel = TRUE
)
rf.fit.bf
rf.pred.bf = predict(rf.fit.bf, df.val)
accuracy(actual = df.val$BF, predicted = rf.pred.bf)
rm(list = ls())
### LIBRARIES ###
library(dplyr)
library(Metrics)
library(rpart)
library(doParallel)
library(rpart.plot)
library(fastDummies)
library(ggplot2)
library(elasticnet)
library(lars)
library(caret)
library(AppliedPredictiveModeling)
library(e1071)
library(pls)
library(plotly)
## LOADING DATA
setwd("C:/Users/David/Google Drive/Github/task3-2-wifi-dgibert17")
load(file = "training.Rdata")
load(file = "validation.Rdata")
# cl <- makeCluster(3)
# doParallel:::registerDoParallel(cl)
bf.vec = createDataPartition(y = df.train$BF, p = 0.15)
# Training data
# bf.tr = df.train[bf.vec$Resample1, c(1:last(grep(pattern = "WAP", names(df.train))), which(names(df.train) == "BF"))]
bf.tr = df.train[bf.vec$Resample1, c(1:last(grep(pattern = "WAP", names(df.train))), 316,322)]
bf.tr$BUILDINGID = factor(bf.tr$BUILDINGID)
df.val$BUILDINGID = factor(df.val$BUILDINGID)
# Test data
# VALIDATION DF
# Tabla de proporciones para ver que la proporcion de observaciones por BUILDING es la misma en el DF original que en el bf.tr
prop.table(table(df.train$BF))
prop.table(table(bf.tr$BF))
ctrl = trainControl(method = "repeatedcv",
number = 3,
repeats = 10)
knn.fit.bf = train(BF~.,
method = "knn",
trControl = ctrl,
data = bf.tr)
knn.fit.bf
knn.pred.bf = predict(knn.fit.bf, df.val)
accuracy(actual = df.val$BF, predicted = knn.pred.bf)
#Cuantas observaciones hay predichas por edificio y cuantas hay en realidad en total.
table(knn.pred.bf)
table(df.val$BF)
#Cuantos dispositivos se han clasificado mal por cada edificio.
abs(table(knn.pred.bf) - table(df.val$BF))
svmLin.fit.bf <- train(BF~.,
method = "svmLinear",
trControl = ctrl,
data = bf.tr
)
svmLin.fit.bf
svmLin.pred.bf = predict(svmLin.fit.bf, df.val)
accuracy(actual = df.val$BF, predicted = svmLin.pred.bf)
svmRad.fit.bf <- train(BF~.,
method = "svmRadial",
trControl = ctrl,
data = bf.tr
)
svmRad.fit.bf
svmRad.pred.bf = predict(svmRad.fit.bf, df.val)
accuracy(actual = df.val$BF, predicted = svmRad.pred.bf)
rf.fit.bf = train(BF~.,
data = bf.tr,
method = "rf",
ntree = 40,
trControl = ctrl,
allowParallel = TRUE
)
rf.fit.bf
rf.pred.bf = predict(rf.fit.bf, df.val)
accuracy(actual = df.val$BF, predicted = rf.pred.bf)
names(bf.tr)
str(bf.tr$BF)
str(bf.tr$BUILDINGID)
str(df.val$BUILDINGID)
str(df.val$BF)
rm(list = ls())
### LIBRARIES ###
library(dplyr)
library(Metrics)
library(rpart)
library(rpart.plot)
library(fastDummies)
library(ggplot2)
library(elasticnet)
library(lars)
library(caret)
library(AppliedPredictiveModeling)
library(e1071)
library(pls)
library(plotly)
### LOADING DATA ###
setwd("C:/Users/David/Google Drive/Github/task3-2-wifi-dgibert17")
df.train = read.csv("training.csv")
df.val = read.csv("validation.csv")
sum(names(df.train) == names(df.val))
df.train$BF = paste0("B",df.train$BUILDINGID,"F",df.train$FLOOR)
df.train$BF = as.factor(df.train$BF)
table(df.train$BF)
df.val$BF = paste0("B",df.val$BUILDINGID,"F",df.val$FLOOR)
df.val$BF = as.factor(df.val$BF)
table(df.val$BF)
## DISTRIBUTION OF THE DATA ON TRAINING AND VALIDATION
barplot(table(df.train$BUILDINGID))
barplot(table(df.train$FLOOR))
barplot(table(df.train$BF))
barplot(table(df.val$BUILDINGID))
barplot(table(df.val$FLOOR))
barplot(table(df.val$BF))
WAPnotDetected.val = as.character()
WAPnotDetected.tr = as.character()
# VALIDATION DF
for (i in 1:which(names(df.val)=="WAP520")){
if (sum(df.val[i]) == 100*nrow(df.val)){
#cat("El", names(df.val[i]), "no ha sido detectado por ningun dispositivo en el validation\n")
WAPnotDetected.val = c(WAPnotDetected.val, names(df.val[i]))
}
}
# TRAINING DF
for (i in 1:which(names(df.train)=="WAP520")){
if (sum(df.train[i]) == 100*nrow(df.train)){
#cat("El", names(df.train[i]), "no ha sido detectado por ningun dispositivo en el training\n")
WAPnotDetected.tr = c(WAPnotDetected.tr, names(df.train[i]))
}
}
WAPnotDetected.val
which(names(df.val) %in% WAPnotDetected.val) #Estos son los indices de los atributos que no son detectados en el validation set, por lo que quitaremos estos atributos del validation set.
df.val = df.val[, -which(names(df.val) %in% WAPnotDetected.val)]
df.train = df.train[, -which(names(df.train) %in% WAPnotDetected.tr)]
tr.in.val.idxs = which(names(df.train) %in% names(df.val))
df.train = df.train[, tr.in.val.idxs]
val.in.tr.idxs = which(names(df.val) %in% names(df.train))
df.val = df.val[, val.in.tr.idxs]
sum(names(df.train) == names(df.val))
# intersect(x = names(df.train), y = names(df.val))
DEVICEnotDetected.tr = c()
DEVICEdetected1WAP.tr = c()
for (i in 1:nrow(df.train)){
if (sum(df.train[i, 1:last(grep(pattern = "WAP", names(df.train)))]) == 100*last(grep(pattern = "WAP", names(df.train)))){
#print(rownames(df.train[i,]))
DEVICEnotDetected.tr = c(DEVICEnotDetected.tr, as.numeric(rownames(df.train[i,])))
} else if (sum(df.train[i, 1:last(grep(pattern = "WAP", names(df.train)))] != 100) == 1){
#print(rownames(df.train[i,]))
DEVICEdetected1WAP.tr = c(DEVICEdetected1WAP.tr, as.numeric(rownames(df.train[i,])))
}
}
# names(df.train[first(grep("[^WAP0-9]", names(df.train)))])
df.DEVICESnotDetected.tr = df.train[DEVICEnotDetected.tr, 313:length(names(df.train))]
df.DEVICEdetected1WAP.tr = df.train[DEVICEdetected1WAP.tr, 313:length(names(df.train))]
DEVICESrm.tr = sort(c(DEVICEnotDetected.tr, DEVICEdetected1WAP.tr))
df.train = df.train[-DEVICESrm.tr, ]
DEVICEnotDetected.val = c()
DEVICEdetected1WAP.val = c()
for (i in 1:nrow(df.val)){
if (sum(df.val[i, 1:last(grep(pattern = "WAP", names(df.val)))]) == 100*last(grep(pattern = "WAP", names(df.val)))){
#print(rownames(df.val[i,]))
DEVICEnotDetected.val = c(DEVICEnotDetected.val, as.numeric(rownames(df.val[i,])))
} else if (sum(df.val[i, 1:last(grep(pattern = "WAP", names(df.val)))] != 100) == 1){
#print(rownames(df.val[i,]))
DEVICEdetected1WAP.val = c(DEVICEdetected1WAP.val, as.numeric(rownames(df.val[i,])))
}
}
DEVICEnotDetected.val
DEVICEdetected1WAP.val
df.val = df.val[-DEVICEdetected1WAP.val, ]
# BUSCAMOS EN TRAINING
cptr30 = c()
WAPs30 = c()
for (i in 1:nrow(df.train[,1:312])){
for (j in 1:ncol(df.train[,1:312])){
if (df.train[i,j] <= 0 & df.train[i,j] >= -30){
cptr30 = c(cptr30,i)
WAPs30 = c(WAPs30,j)
}
}
}
cptr30 = unique(as.vector(cptr30))
WAPs30 = unique(as.vector(WAPs30))
df.cptr30 = df.train[cptr30,]
df.WAPs30 = df.train[,WAPs30]
# df.cptr30 = df.cptr30[,c(315:316, 319:320, 322)]
barplot(table(df.cptr30$USERID)) #El usuario que tiene mas errores de medicion es el USERID 6, seguido del 14.
barplot(table(df.cptr30$BF))
df.cptr30$FLOOR = as.factor(df.cptr30$FLOOR)
df.cptr30$BUILDINGID = as.factor(df.cptr30$BUILDINGID)
df.cptr30 %>%
select(USERID, PHONEID, BF) %>%
filter(USERID == 6) %>%
summary()
df.cptr30 %>%
select(USERID, PHONEID, BF) %>%
filter(USERID == 14) %>%
summary()
vec.cptr30 = as.integer(row.names(df.cptr30))
table(df.train$BF)
table(df.train[-vec.cptr30,]$BF)
df.train = df.train[-vec.cptr30, ]
duplicated.rows = df.train[duplicated(df.train), ]
df.train = df.train[!duplicated(df.train), ]
### CREATING TRAIN BUILDINGS - PIPES ###
df.trainB0 = df.train %>%
filter(BUILDINGID == 0)
df.trainB1 = df.train %>%
filter(BUILDINGID == 1)
df.trainB2 = df.train %>%
filter(BUILDINGID == 2)
a <- list(
title = "",
zeroline = FALSE,
showline = FALSE,
showticklabels = FALSE,
showgrid = FALSE
)
# All buildings, 2d
plot_ly(data = df.train, x = ~LONGITUDE, y = ~LATITUDE, color = ~factor(BUILDINGID)) %>%
layout(xaxis = a,
yaxis = a)
# All buildings, 3d
plot_ly(data = df.train, x = ~LONGITUDE, y = ~LATITUDE, z = ~factor(FLOOR),
color = ~factor(BUILDINGID)) %>%
layout(xaxis = a,
yaxis = a)
# Building 0, 3d
plot_ly(data = df.trainB0, x = ~LONGITUDE, y = ~LATITUDE, z = ~factor(FLOOR),
color = ~factor(FLOOR)) %>%
layout(title = "Building 0",
xaxis = a,
yaxis = a
)
# Building 1, 3d
plot_ly(data = df.trainB1, x = ~LONGITUDE, y = ~LATITUDE, z = ~factor(FLOOR),
color = ~factor(FLOOR)) %>%
layout(title = "Building 1",
xaxis = a,
yaxis = a
)
# Building 2, 3d
plot_ly(data = df.trainB2, x = ~LONGITUDE, y = ~LATITUDE, z = ~factor(FLOOR),
color = ~factor(FLOOR)) %>%
layout(title = "Building 2",
xaxis = a,
yaxis = a
)
save(df.train, file = "training.Rdata")
save(df.val, file = "validation.Rdata")
table(df.trainB0$WAP001)
table(df.trainB1$WAP001)
table(df.trainB2$WAP001)
names(df.train)
DEVICEnotDetected.tr = c()
DEVICEdetected1WAP.tr = c()
for (i in 1:nrow(df.train)){
if (sum(df.train[i, 1:last(grep(pattern = "WAP", names(df.train)))]) == 100*last(grep(pattern = "WAP", names(df.train)))){
#print(rownames(df.train[i,]))
DEVICEnotDetected.tr = c(DEVICEnotDetected.tr, as.numeric(rownames(df.train[i,])))
} else if (sum(df.train[i, 1:last(grep(pattern = "WAP", names(df.train)))] != 100) == 1){
#print(rownames(df.train[i,]))
DEVICEdetected1WAP.tr = c(DEVICEdetected1WAP.tr, as.numeric(rownames(df.train[i,])))
}
}
# names(df.train[first(grep("[^WAP0-9]", names(df.train)))])
df.DEVICESnotDetected.tr = df.train[DEVICEnotDetected.tr, 313:length(names(df.train))]
df.DEVICEdetected1WAP.tr = df.train[DEVICEdetected1WAP.tr, 313:length(names(df.train))]
DEVICESrm.tr = sort(c(DEVICEnotDetected.tr, DEVICEdetected1WAP.tr))
df.train = df.train[-DEVICESrm.tr, ]
