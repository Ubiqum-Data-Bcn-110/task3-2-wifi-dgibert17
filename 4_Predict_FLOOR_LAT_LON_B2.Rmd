---
title: "Wi-Fi positioning system. FLOOR/LAT/LONG predictions on Building 2"
author: "David Gibert Bosque"
date: "January 4th, 2019"
output:
  rmdformats::readthedown:
    self_contained: false
    thumbnails: true
    lightbox: true
    gallery: true
    highlight: tango
---
```{r include=FALSE}
rm(list = ls())
```

```{r message=FALSE, warning=FALSE, include=FALSE}
library(dplyr)
library(Metrics)
library(rpart)
library(rpart.plot)
library(fastDummies)
library(ggplot2)
library(elasticnet)
library(randomForest)
library(lars)
library(caret)
library(AppliedPredictiveModeling)
library(RColorBrewer)
library(e1071)
library(pls)
library(plotly)
library(class)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
### LOADING DATA ###
setwd("C:/Users/David/Google Drive/Github/task3-2-wifi-dgibert17")

load(file = "training.Rdata")
load(file = "validation.Rdata")
```

**Bold text in English.**

*Texto en cursiva en EspaÃ±ol.*

## VALIDATION & TRAINING datasets for building 2
```{r}
df.train.B0 = df.train %>%
  filter(BUILDINGID == 2)

df.val.B0 = df.val %>%
  filter(BUILDINGID == 2)
```

**Because we have filtered the data by building, we have to do once again the pre-processing carried out previously. Removing undetected WAP's and locate the intersection of attributes of both data sets is a must before starting the machine learning process.**

*Debido a que hemos filtrado por edificio, tenemos que volver a hacer el pre-procesado de datos que hemos llevado a cabo anteriormente. Hay que quitar WAPs no detectados y localizar la interseccion de atributos de ambos datasets antes de empezar con el procedimiento de machine learning.*

---

## Non detected WAP's

**Locating WAP's that have not been detected by any device.**

*Localizamos los WAPs que no han sido detectados por ningun dispositivo.*
```{r}
WAPnotDetected.val = as.character()
WAPnotDetected.tr = as.character()

# VALIDATION DF

for (i in 1:which(names(df.val.B0)=="WAP508")){
  if (sum(df.val.B0[i]) == 100*nrow(df.val.B0)){
    #cat("El", names(df.val[i]), "no ha sido detectado por ningun dispositivo en el validation\n")
    WAPnotDetected.val = c(WAPnotDetected.val, names(df.val.B0[i]))
  }
}

# TRAINING DF

for (i in 1:which(names(df.train.B0)=="WAP508")){
  if (sum(df.train.B0[i]) == 100*nrow(df.train.B0)){
    #cat("El", names(df.train[i]), "no ha sido detectado por ningun dispositivo en el training\n")
    WAPnotDetected.tr = c(WAPnotDetected.tr, names(df.train.B0[i]))
  }
}
```

## Removing non detected WAP's
```{r}
WAPnotDetected.val
# which(names(df.val.B0) %in% WAPnotDetected.val) #Estos son los indices de los atributos que no son detectados en el validation set, por lo que quitaremos estos atributos del validation set.

df.val.B0 = df.val.B0[, -which(names(df.val.B0) %in% WAPnotDetected.val)]
df.train.B0 = df.train.B0[, -which(names(df.train.B0) %in% WAPnotDetected.tr)]
```

---

**Looking for the intersect of attributes appearing in both datasets. Attributes in VALIDATION that appear in TRAINING and vice versa. VALIDATION attributes that appear in TRAINING first.**

*Buscamos la interseccion de los atributos que aparecen en ambos conjuntos de datos. Atributos en VALILDATION que aparecen en TRAINING y viceversa. Los atributos de VALIDATION que aparecen en TRAINING primero.*
```{r}
tr.in.val.idxs = which(names(df.train.B0) %in% names(df.val.B0))
df.train.B0 = df.train.B0[, tr.in.val.idxs]

val.in.tr.idxs = which(names(df.val.B0) %in% names(df.train.B0))
df.val.B0 = df.val.B0[, val.in.tr.idxs]

sum(names(df.train.B0) == names(df.val.B0))

# intersect(x = names(df.train), y = names(df.val))
```

#### Formatting categorical data
```{r}
df.train.B0$FLOOR = factor(df.train.B0$FLOOR)
df.train.B0$BUILDINGID = factor(df.train.B0$BUILDINGID)

real.values = data.frame(df.val.B0[,c(107:109)])

df.val.B0$FLOOR = factor(df.val.B0$FLOOR)
df.val.B0$BUILDINGID = factor(df.val.B0$BUILDINGID)
```

---

## Data partition by Floor for Building 2
```{r}
floor.vec = createDataPartition(y = df.train.B0$FLOOR, times = 5, p = 0.1)

# Training data
floor.tr = df.train.B0[, c(1:last(grep(pattern = "WAP", names(df.train.B0))), which(names(df.train.B0) == "FLOOR"))]
floor.tr$FLOOR = factor(floor.tr$FLOOR)

#Proporcion de datos en TRAIN y SAMPLE
prop.table(table(df.train.B0$FLOOR))
prop.table(table(floor.tr$FLOOR))
```

## Floor prediction - KNN
```{r warning=FALSE}
knn.fit.floor = knn(train = floor.tr[,1:106], test = df.val.B0[,c(1:106)], cl = floor.tr[,107], k = 5)
confusionMatrix(data = factor(knn.fit.floor), reference = df.val.B0$FLOOR)

#Cuantos dispositivos se han clasificado mal por cada edificio.
abs(table(knn.fit.floor) - table(df.val.B0$FLOOR))
```

## Floor prediction - SVM
```{r warning=FALSE}
svm.fit.floor <- svm(formula = FLOOR ~ ., data = floor.tr)
svm.pred.floor <- predict(svm.fit.floor, newdata = df.val.B0[,c(1:106)])

confusionMatrix(data = svm.pred.floor, reference = df.val.B0$FLOOR)

# #Cuantos dispositivos se han clasificado mal por cada edificio.
abs(table(svm.pred.floor) - table(df.val.B0$FLOOR))
```

## Floor prediction - Random Forest
```{r}
# bestmtry = tuneRF(x = floor.tr[,1:106], y = floor.tr$FLOOR, ntreeTry = 400, plot = T,stepFactor = 1.5, improve = 1e-5)

rf.fit.floor = randomForest(x = floor.tr[,1:106], y = floor.tr$FLOOR, ntree = 400, mtry = 15)
rf.pred.floor = predict(rf.fit.floor, df.val.B0)
confusionMatrix(data = factor(rf.pred.floor), reference = df.val.B0$FLOOR)

#Cuantos dispositivos se han clasificado mal por cada Planta
abs(table(rf.pred.floor) - table(df.val.B0$FLOOR))
```

**The method that produces a prediction with less error is Random Forest.**

*El metodo que produce una prediccion con menor error es Random Forest.*

---

#### Replacing actual Floor values for predicted ones
```{r}
df.val.B0$FLOOR = rf.pred.floor
str(df.val.B0$FLOOR)
```

---

**After making different combinations, the lowest prediction error is achieved by first predicting Latitude with dummified Floor and Longitude later, using the information provided by Latitude.**

*Despues de realizar diferentes combinaciones, el error de prediccion mas bajo se consigue prediciendo primero Latitude con la variable 'Dummy' de Floor y Longitude despues, usando la informacion que proporciona la variable Latitude.*

## Dummifying Floor
```{r}
pacman::p_load(fastDummies)

df.val.B0.dumm = dummy_cols(.data = df.val.B0, select_columns = "FLOOR", remove_first_dummy = FALSE)
df.val.B0.dumm = select(.data = df.val.B0.dumm, -FLOOR)

df.train.B0.dumm = dummy_cols(.data = df.train.B0, select_columns = "FLOOR", remove_first_dummy = FALSE)
df.train.B0.dumm = select(.data = df.train.B0.dumm, -FLOOR)
```

## Latitude prediction - SVM
```{r warning=FALSE}
svm.fit.lat <- svm(formula = LATITUDE ~ ., data = df.train.B0.dumm[,c(1:106, 108, 116:120)])
svm.pred.lat <- predict(svm.fit.lat, newdata = df.val.B0.dumm[,c(1:106, 116:120)])

rmse(actual = df.val.B0.dumm$LATITUDE, predicted = svm.pred.lat)
mae(actual = df.val.B0.dumm$LATITUDE, predicted = svm.pred.lat)
```

## Latitude prediction - Linear Model
```{r warning=FALSE}
lm.fit.lat <- lm(formula = LATITUDE ~ ., data = df.train.B0.dumm[,c(1:106, 108, 116:120)])
lm.pred.lat <- predict(lm.fit.lat, newdata = df.val.B0.dumm[,c(1:106, 116:120)])

rmse(actual = df.val.B0.dumm$LATITUDE, predicted = lm.pred.lat)
mae(actual = df.val.B0.dumm$LATITUDE, predicted = lm.pred.lat)
```

## Latitude prediction - Random Forest
```{r}
# bestmtry = tuneRF(x = df.train.B0.dumm[,c(1:106, 116:120)], y = df.train.B0.dumm$LATITUDE, ntreeTry = 400, plot = T,stepFactor = 1.5, improve = 1e-5)

rf.fit.lat = randomForest(LATITUDE ~., df.train.B0.dumm[,c(1:106, 108, 116:120)], ntree = 150, mtry = 37)

rf.pred.lat = predict(rf.fit.lat, df.val.B0.dumm[,c(1:106, 116:120)])

rmse(actual = df.val.B0.dumm$LATITUDE, predicted = rf.pred.lat)
mae(actual = df.val.B0.dumm$LATITUDE, predicted = rf.pred.lat)
```

## Latitude prediction - KNN
```{r warning=FALSE}
pacman::p_load(FNN)

knn.fit.lat = knn.reg(train = df.train.B0.dumm[,c(1:106, 116:120)], test = df.val.B0.dumm[,c(1:106, 116:120)],
                       y = df.train.B0.dumm[, 108], k = 5,
                       algorithm = c("kd_tree", "cover_tree", "brute"))

rmse(actual = df.val.B0.dumm$LATITUDE, predicted = knn.fit.lat$pred)
mae(actual = df.val.B0.dumm$LATITUDE, predicted = knn.fit.lat$pred)
```

**The method that produces a prediction with less error is Random Forest.**

*El metodo que produce una prediccion con menor error es Random Forest.*

---

#### Replacing actual Latitude values for predicted ones
```{r}
df.val.B0.dumm$LATITUDE = rf.pred.lat
```

---

## Longitude prediction - SVM
```{r warning=FALSE}
svm.fit.long <- svm(formula = LONGITUDE ~ ., data = df.train.B0.dumm[,c(1:106, 107, 108, 116:120)])
svm.pred.long <- predict(svm.fit.long, newdata = df.val.B0.dumm[,c(1:106, 108, 116:120)])

rmse(actual = df.val.B0.dumm$LONGITUDE, predicted = svm.pred.long)
mae(actual = df.val.B0.dumm$LONGITUDE, predicted = svm.pred.long)
```

## Longitude prediction - Linear Model
```{r warning=FALSE}
lm.fit.long <- lm(formula = LONGITUDE ~ ., data = df.train.B0.dumm[,c(1:106, 107, 108, 116:120)])
lm.pred.long <- predict(lm.fit.long, newdata = df.val.B0.dumm[,c(1:106, 108, 116:120)])

rmse(actual = df.val.B0.dumm$LONGITUDE, predicted = lm.pred.long)
mae(actual = df.val.B0.dumm$LONGITUDE, predicted = lm.pred.long)
```

## Longitude prediction - Random Forest
```{r}
# bestmtry = tuneRF(x = df.train.B0.dumm[,c(1:106, 108, 116:120)], y = df.train.B0.dumm$LONGITUDE, ntreeTry = 150, plot = T,stepFactor = 1.5, improve = 1e-5)

rf.fit.long = randomForest(LONGITUDE ~., df.train.B0.dumm[,c(1:106, 107, 108, 116:120)], ntree = 150, mtry = 82)

rf.pred.long = predict(rf.fit.long, df.val.B0.dumm[,c(1:106, 108, 116:120)])

rmse(actual = df.val.B0.dumm$LONGITUDE, predicted = rf.pred.long)
mae(actual = df.val.B0.dumm$LONGITUDE, predicted = rf.pred.long)
```

## Longitude prediction - KNN
```{r warning=FALSE}
pacman::p_load(FNN)

knn.fit.long = knn.reg(train = df.train.B0.dumm[,c(1:106, 108, 116:120)], test = df.val.B0.dumm[,c(1:106, 108, 116:120)],
                       y = df.train.B0.dumm[,107], k = 5,
                       algorithm = c("kd_tree", "cover_tree", "brute"))

rmse(actual = df.val.B0.dumm$LONGITUDE, predicted = knn.fit.long$pred)
mae(actual = df.val.B0.dumm$LONGITUDE, predicted = knn.fit.long$pred)
```

**KNN is the best predictive method in this building, in terms of error rate and computational level. It performs better and much faster than Random Forest.**

*KNN es el mejor metodo predictivo en este edificio, en terminos de tasa de error y nivel computacional. Su desempeÃ±o es mejor y mucho mas rapido que el de Random Forest.*

---

#### Replacing actual Longitude values for predicted ones
```{r}
df.val.B0.dumm$LONGITUDE = knn.fit.long$pred
```

---

#### ANALISIS DEL ERROR DE PREDICCION
PLOTEAR EN 3D VALIDATION Y TAL...